{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs0yamHN-dY0"
      },
      "source": [
        "# **CIS 4190/5190 Spring 2023 - Homework 2**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Rb-WLp5Z-cdy"
      },
      "outputs": [],
      "source": [
        "import random \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.linalg import *\n",
        "np.random.seed(42)  # don't change this line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "O2VtEzsZ-loR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72078d28-b45f-4aca-984b-26093a597eb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO, OK] Google Colab.\n"
          ]
        }
      ],
      "source": [
        "# For autogreader only, do not modify this cell. \n",
        "# True for Google Colab, False for autograder\n",
        "NOTEBOOK = (os.getenv('IS_AUTOGRADER') is None)\n",
        "if NOTEBOOK:\n",
        "    print(\"[INFO, OK] Google Colab.\")\n",
        "else:\n",
        "    print(\"[INFO, OK] Autograder.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjjXBdEb-p8K"
      },
      "source": [
        "# **PennGrader Setup**\n",
        "\n",
        "First, you'll need to set up the PennGrader, an autograder we are going to use throughout the semester. The PennGrader will automatically grade your answer and provide you with an instant feedback. Unless otherwise stated, you can resubmit up to a reasonable number of attempts (e.g. 100 attemptes per day). **We will only record your latest score in our backend database**. \n",
        "\n",
        "After finishing each homework assignment, you must submit your iPython notebook to gradescope before the homework deadline. Gradescope will then retrive and display your scores from our backend database. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6GCTLN4G-nK2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip3 install penngrader --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QLnoPRci-sTC"
      },
      "outputs": [],
      "source": [
        "from penngrader.grader import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qu0XYZHO-t8J"
      },
      "outputs": [],
      "source": [
        "#PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO \n",
        "#TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
        "STUDENT_ID = 82897132          # YOUR PENN-ID GOES HERE AS AN INTEGER#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIDTGGbo-xkf"
      },
      "source": [
        "Run the following cell to initialize the autograder. This autograder will let you submit your code directly from this notebook and immidiately get a score.\n",
        "\n",
        "**NOTE:** Remember we store your submissions and check against other student's submissions... so, not that you would, but no cheating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bw_QDnZk-vvI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba6d76a8-11f4-41e0-c542-a6c80926d47d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PennGrader initialized with Student ID: 82897132\n",
            "\n",
            "Make sure this correct or we will not be able to store your grade\n"
          ]
        }
      ],
      "source": [
        "grader = PennGrader(homework_id = 'CIS5190_Sp23_HW2', student_id = STUDENT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "f0_ydbgD0Kvf"
      },
      "outputs": [],
      "source": [
        "# Serialization code needed by the autograder\n",
        "import inspect, sys\n",
        "from IPython.core.magics.code import extract_symbols\n",
        "\n",
        "def new_getfile(object, _old_getfile=inspect.getfile):\n",
        "    if not inspect.isclass(object):\n",
        "        return _old_getfile(object)\n",
        "    \n",
        "    # Lookup by parent module (as in current inspect)\n",
        "    if hasattr(object, '__module__'):\n",
        "        object_ = sys.modules.get(object.__module__)\n",
        "        if hasattr(object_, '__file__'):\n",
        "            return object_.__file__\n",
        "    \n",
        "    # If parent module is __main__, lookup by methods (NEW)\n",
        "    for name, member in inspect.getmembers(object):\n",
        "        if inspect.isfunction(member) and object.__qualname__ + '.' + member.__name__ == member.__qualname__:\n",
        "            return inspect.getfile(member)\n",
        "    else:\n",
        "        raise TypeError('Source for {!r} not found'.format(object))\n",
        "inspect.getfile = new_getfile\n",
        "\n",
        "def grader_serialize(obj):\n",
        "    cell_code = \"\".join(inspect.linecache.getlines(new_getfile(obj)))\n",
        "    class_code = extract_symbols(cell_code, obj.__name__)[0][0]\n",
        "    return class_code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQCyLhELJ7LX"
      },
      "source": [
        "#### **NOTE 1. Results of sections marked as \"manually graded\" should be submitted along with the written homework solutions.**\n",
        "\n",
        "#### **NOTE 2. If you are running into a `__builtins__' error, it's likely because you're using a function call of the form numpy.ndarray.mean(), like a.mean(). This does not play nice with PennGrader unfortunately. Please use the function call numpy.mean(a) instead.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfs6aWUhmWJT"
      },
      "source": [
        "# **1. Linear Regression**\n",
        "\n",
        "## **1.1. Linear Regression Implementation [19 pts, autograded]**\n",
        "\n",
        "In this section you will implement linear regression with both L1 and L2 regularization. Your class LinearRegression must implement the following API:\n",
        "\n",
        "* `__init__(alpha, tol, max_iter, theta_init, penalty, lambd)`\n",
        "* `compute_cost(theta, X, y)`\n",
        "* `compute_gradient(theta, X, y)`\n",
        "* `fit(X, y)`\n",
        "* `has_converged(theta_old, theta_new)`\n",
        "* `predict(X)`\n",
        "\n",
        "Note that these methods have already been defined correctly for you in the LinearRegression class. **DO NOT** change the API.\n",
        "\n",
        "### **1.1.1. Cost Function [5 pts]**\n",
        "\n",
        "The `compute_cost` function should compute the cost for a given $\\theta$ vector. The cost is a scalar value given by:\n",
        "\n",
        "$\n",
        "\\mathcal{L}({\\theta}) = \\frac{1}{N}\\sum_{i =1}^N (h_{{\\theta}}({x}_i) - y_i)^2\n",
        "$\n",
        "\n",
        "where\n",
        "\n",
        "> $h_{{\\theta}}({x}_i) = \\theta^Tx_i$\n",
        "\n",
        "Based on the regularization penalty, you may need to add below regularization penalty loss to MSE Loss computed previously.\n",
        "\n",
        "L1 Regularization Loss:\n",
        ">$\n",
        "\\mathcal{L_1}({\\theta}) = \\mathcal{L}({\\theta}) + \\lambda\\sum_{j = 1}^D  |{\\theta}_j|\n",
        "$\n",
        "\n",
        "L2 Regularization Loss:\n",
        ">$\n",
        "\\mathcal{L_2}({\\theta}) = \\mathcal{L}({\\theta}) + \\lambda\\sum_{j = 1}^D  {\\theta}_j^2 \n",
        "$\n",
        "\n",
        "$N$ is the number of training samples and $D$ is the number of features (excluding the intercept term). $\\theta$ is a $D + 1$ dimensional vector, with the first element being the intercept term. Note that we do not include the intercept in the regularization terms.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1.2. Gradient of the Cost Function [5 pts]**\n",
        "\n",
        "The `compute_gradient` function should compute the gradient of the cost function at a given $\\theta$.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1.3. Convergence Check [1 pt]**\n",
        "\n",
        "The `has_converged` function should return whether gradient descent algorithm has converged or not. Refer 1.1.4 for convergence condition.\n",
        " \n",
        "---\n",
        "\n",
        "### **1.1.4. Training with Gradient Descent [3 pts]**\n",
        "\n",
        "The `fit` method should train the model via gradient descent, relying on the `compute_cost` and `compute_gradient` functions. The trained weights/coefficients must be stored as `theta_`. The weights and the corresponding cost after every gradient descent iteration must be stored in `hist_theta_` and `hist_cost_` respectively.\n",
        "\n",
        "* The gradient descent stops or converges when $\\theta$ stops changing or changes negligibly between consecutive iterations, i.e., when \n",
        "$\\| {\\theta}_\\mathit{new} -  {\\theta}_\\mathit{old} \\|_2 \\leq \\epsilon$, \n",
        "for some small $\\epsilon$ (e.g., $\\epsilon$ = 1E-4). $\\epsilon$ is stored as `tol` (short for tolerance). \n",
        "\n",
        "* To ensure that the function terminates, we should set a maximum limit for the number of epochs irrespective of whether $\\theta$ converges or not. The limit is stored as `max_iter`.\n",
        "\n",
        "* `alpha` is the learning rate of the gradient descent algorithm.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1.5. Training with Stochastic Gradient Descent (SGD) [3 pts]**\n",
        "\n",
        "The `fit_sgd` method should train the model via stochastic gradient descent (SGD), relying on the `compute_cost` and `compute_gradient` functions.\n",
        "\n",
        "The trained weights/coefficients must be stored as `theta_`. The weights and the corresponding cost after every SGD iteration must be stored in `hist_theta_` and `hist_cost_` respectively.\n",
        "\n",
        "* Unlike regular (or batch) gradient descent, SGD takes a gradient step on a single training example on each iteration. In other words, rather than compute the gradient for all training examples, summing them, and taking a single gradient step, it iterates through training examples, computes the gradient for that training example, and immediately takes a single gradient step before proceeding to the next training example. One pass over the entire training dataset is called an epoch; at the end of an epoch, the next epoch restarts from the first example in the training dataset.\n",
        "\n",
        "* As with gradient descent, SGD stops or converges when $\\theta$ stops changing or changes negligibly between consecutive iterations, i.e., when \n",
        "$\\| {\\theta}_\\mathit{new} -  {\\theta}_\\mathit{old} \\|_2 \\leq \\epsilon$, \n",
        "for some small $\\epsilon$ (e.g., $\\epsilon$ = 1E-4). $\\epsilon$ is stored as `tol` (short for tolerance). Since each step is much shorter, SGD typically only checks for convergence at the end of each epoch.\n",
        "\n",
        "* To ensure that the function terminates, we should set a maximum limit for the number of gradient descent iterations irrespective of whether $\\theta$ converges or not. The limit is stored as `max_iter`.\n",
        "\n",
        "* `alpha` is the learning rate of the SGD algorithm.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1.6. Predict [2 pts]**\n",
        "\n",
        "The `predict` function should predict the output for the data points in a given input data matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "x_iD4A-TmjKe"
      },
      "outputs": [],
      "source": [
        "class LinearRegression:\n",
        "\n",
        "    \"\"\"\n",
        "    Linear Regression\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha: float, default=0.01\n",
        "        Learning rate\n",
        "    tol : float, default=0.0001\n",
        "        Tolerance for stopping criteria\n",
        "    max_iter : int, default=10000\n",
        "        Maximum number of iterations of gradient descent\n",
        "    theta_init: None (or) numpy.ndarray of shape (D + 1,)\n",
        "        The initial weights; if None, all weights will be zero by default\n",
        "    penalty : string, default = None\n",
        "        The type of regularization. The other acceptable options are l1 and l2\n",
        "    lambd : float, default = 1.0\n",
        "        The parameter regularization constant (i.e. lambda)\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    theta_ : numpy.ndarray of shape (D + 1,)\n",
        "        The value of the coefficients after gradient descent has converged\n",
        "        or the number of iterations hit the maximum limit\n",
        "    hist_theta_ : numpy.ndarray of shape (num_iter, D + 1) where num_iter is the number of gradient descent iterations\n",
        "        Stores theta_ after every gradient descent iteration\n",
        "    hist_cost_ : numpy.ndarray of shape (num_iter,) where num_iter is the number of gradient descent iterations\n",
        "        Stores cost after every gradient descent iteration\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, alpha = 0.01, tol=1e-4, max_iter = 100, theta_init = None, penalty = None, lambd = 0):\n",
        "        \n",
        "        # store meta-data\n",
        "        self.alpha = alpha\n",
        "        self.theta_init = theta_init\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.penalty = penalty\n",
        "        self.lambd = lambd\n",
        "\n",
        "        self.theta_ = None\n",
        "        self.hist_cost_ = None\n",
        "        self.hist_theta_ = None\n",
        "    \n",
        "    def compute_cost(self, theta, X, y):\n",
        "    \n",
        "        \"\"\"\n",
        "        Compute the cost/objective function.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta: numpy.ndarray of shape (D + 1,)\n",
        "            The coefficients\n",
        "        X: numpy.ndarray of shape (N, D + 1)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        cost: float\n",
        "            The cost as a scalar value\n",
        "        \"\"\"\n",
        "        \n",
        "        # TODO STARTS: Complete the function (should account for three cases - no penalty, l1 penalty, and l2 penalty)\n",
        "        #print(theta)\n",
        "        #print(theta[1:])\n",
        "        #𝜃  is a 𝐷+1 dimensional vector, with the first element being the intercept term. Note that we do not include the intercept in the regularization terms.\n",
        "        mse = np.sum((y - np.dot(X, theta)) ** 2) / len(X)\n",
        "        fix = 0\n",
        "        if self.penalty == \"l1\":\n",
        "          fix = self.lambd * np.sum(abs(theta[1:]))\n",
        "        if self.penalty == \"l2\":\n",
        "          fix = self.lambd * np.sum(np.dot(theta[1:], theta[1:]))\n",
        "        res = mse + fix\n",
        "        return res\n",
        "        # TODO ENDS\n",
        "\n",
        "    def compute_gradient(self, theta, X, y):\n",
        "    \n",
        "        \"\"\"\n",
        "        Compute the gradient of the cost function.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta: numpy.ndarray of shape (D + 1,)\n",
        "            The coefficients\n",
        "        X: numpy.ndarray of shape (N, D + 1)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        gradient: numpy.ndarray of shape (D + 1,)\n",
        "            The gradient values\n",
        "        \"\"\"\n",
        "        \n",
        "        # TODO STARTS: Complete the function (should account for three cases - no penalty, l1 penalty, and l2 penalty)\n",
        "        #print(\"theta\")\n",
        "        #print(theta)\n",
        "        #print(theta.shape)\n",
        "        #print(\"y shape\")\n",
        "        #print(y)\n",
        "        #print(y.shape)\n",
        "        diff = (y - np.matmul(X, theta)) # (n, )\n",
        "        #print(diff)\n",
        "        #print(X)\n",
        "        #print(np.matmul(diff, X))\n",
        "        #print(\"diff shape\")\n",
        "        #print(diff)\n",
        "        #print(diff.shape)\n",
        "        #print(\"X shape\")\n",
        "        #print(X)\n",
        "        #print(X.shape)\n",
        "        grad = -2 * np.matmul(diff, X) / len(X) # (d + 1, )\n",
        "        #print(grad)\n",
        "        fix = 0\n",
        "        if self.penalty == \"l1\":\n",
        "            grad[1:] += self.lambd * np.sign(theta[1:]) # (d, )\n",
        "        if self.penalty == \"l2\":\n",
        "            grad[1:] += 2 * self.lambd * theta[1:]\n",
        "        \n",
        "        return grad #(d + 1, )\n",
        "        # TODO ENDS\n",
        "\n",
        "    def has_converged(self, theta_old, theta_new):\n",
        "\n",
        "        \"\"\"\n",
        "        Return whether gradient descent has converged.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta_old: numpy.ndarray of shape (D + 1,)\n",
        "            The weights prior to the update by gradient descent\n",
        "        theta_new: numpy.ndarray of shape (D + 1,)\n",
        "            The weights after the update by gradient descent\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        converged: bool\n",
        "            Whether gradient descent converged or not\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO START: Complete the function\n",
        "        converge = np.sqrt(np.matmul(theta_old - theta_new, theta_old - theta_new))\n",
        "        if converge < self.tol:\n",
        "          return True\n",
        "        return False\n",
        "        # TODO END\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the coefficients using gradient descent and store them as theta_.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Nothing\n",
        "        \"\"\"\n",
        "\n",
        "        N, D = X.shape\n",
        "\n",
        "        # Adding a column of ones at the beginning for the bias term\n",
        "        ones_col = np.ones((N, 1))\n",
        "        X = np.hstack((ones_col, X))\n",
        "        \n",
        "        # Initializing the weights\n",
        "        if self.theta_init is None:\n",
        "            theta_old = np.zeros((D + 1,))\n",
        "        else:\n",
        "            theta_old = self.theta_init\n",
        "\n",
        "        # Initializing the historical weights matrix\n",
        "        # Remember to append this matrix with the weights after every gradient descent iteration\n",
        "        self.hist_theta_ = np.array([theta_old])\n",
        "\n",
        "        # Computing the cost for the initial weights\n",
        "        cost = self.compute_cost(theta_old, X, y)\n",
        "\n",
        "        # Initializing the historical cost array\n",
        "        # Remember to append this array with the cost after every gradient descent iteration\n",
        "        self.hist_cost_ = np.array([cost])\n",
        "        \n",
        "        # TODO START: Complete the function\n",
        "        for i in range(self.max_iter):\n",
        "          theta_ = theta_old - self.alpha * self.compute_gradient(theta_old, X, y)\n",
        "          self.hist_theta_ = np.vstack((self.hist_theta_, theta_))\n",
        "          self.hist_cost_ = np.vstack((self.hist_cost_, self.compute_cost(theta_, X, y)))\n",
        "          if self.has_converged(theta_old, theta_):\n",
        "            break \n",
        "          theta_old = theta_\n",
        "        # TODO END\n",
        "\n",
        "    def fit_sgd(self, X, y):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the coefficients using gradient descent and store them as theta_.\n",
        "        Make sure to save theta_ for each gradient descent conducted\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Nothing\n",
        "        \"\"\"\n",
        "\n",
        "        N, D = X.shape\n",
        "\n",
        "        # Adding a column of ones at the beginning for the bias term\n",
        "        ones_col = np.ones((N, 1))\n",
        "        X = np.hstack((ones_col, X))\n",
        "        \n",
        "        # Initializing the weights\n",
        "        if self.theta_init is None:\n",
        "            theta_old = np.zeros((D + 1,))\n",
        "        else:\n",
        "            theta_old = self.theta_init\n",
        "\n",
        "        # Initializing the historical weights matrix\n",
        "        # Remember to append this matrix with the weights after every gradient descent iteration\n",
        "        self.hist_theta_ = np.array([theta_old])\n",
        "\n",
        "        # Computing the cost for the initial weights\n",
        "        cost = self.compute_cost(theta_old, X, y)\n",
        "\n",
        "        # Initializing the historical cost array\n",
        "        # Remember to append this array with the cost after every gradient descent iteration\n",
        "        self.hist_cost_ = np.array([cost])\n",
        "        \n",
        "        # TODO START: Complete the function\n",
        "        for i in range(self.max_iter):\n",
        "          for j in range(N):\n",
        "            x_i = np.array([X[j,:]])\n",
        "            y_i = np.array([y[j]])\n",
        "            theta_ = theta_old - self.alpha * self.compute_gradient(theta_old, x_i, y_i)\n",
        "            self.hist_theta_ = np.vstack((self.hist_theta_, theta_))\n",
        "            self.hist_cost_ = np.vstack((self.hist_cost_, self.compute_cost(theta_, x_i, y_i)))\n",
        "            if self.has_converged(theta_old, theta_):\n",
        "              break \n",
        "            theta_old = theta_\n",
        "        # TODO END\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        \"\"\"\n",
        "        Predict the target variable values for the data points in X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_hat: numpy.ndarray of shape (N,)\n",
        "            The predicted target variables values for the data points in X\n",
        "        \"\"\"\n",
        "\n",
        "        N = X.shape[0]\n",
        "        X = np.hstack((np.ones((N, 1)), X))\n",
        "        \n",
        "        # TODO START: Complete the function\n",
        "        return np.matmul(X, self.hist_theta_[-1])\n",
        "        # TODO END"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    test_case_theta = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "student_lr_reg = LinearRegression()\n",
        "#ans = student_lr_reg.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
        "required_ans = [ 4.79663712, -3.53908485]\n",
        "student_lr_reg.compute_gradient(test_case_theta, test_case_X, test_case_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjjeWAcSWz6W",
        "outputId": "15bb7da3-2964-40ef-8b59-31f8e3dbea05"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 4.79663712, -3.53908485])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0gOdODTWQQX0"
      },
      "outputs": [],
      "source": [
        "def test_lin_reg_compute_cost(StudentLinearRegression):\n",
        "    \n",
        "    test_case_theta = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression()\n",
        "    student_ans = student_lr_reg.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = 4.881828654157736\n",
        "    \n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(penalty=\"l1\", lambd=0.1)\n",
        "    student_ans = student_lr_reg.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = 4.94300429515773\n",
        "\n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(penalty=\"l2\", lambd=0.1)\n",
        "    student_ans = student_lr_reg.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = 4.919253244675344\n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_lin_reg_compute_cost(LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ct-hUcbC9Zp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea87c66d-2e7f-47e6-f044-275b0d9a4f09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 5/5 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_lin_reg_compute_cost', answer = LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pysdW3awRLl1"
      },
      "outputs": [],
      "source": [
        "def test_lin_reg_compute_gradient(StudentLinearRegression):\n",
        "    \n",
        "    test_case_theta = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression()\n",
        "    student_ans = student_lr_reg.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = [ 4.79663712, -3.53908485]\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(penalty=\"l1\", lambd=0.1)\n",
        "    student_ans = student_lr_reg.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = [ 4.79663712, -3.63908485]\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(penalty=\"l2\", lambd=0.1)\n",
        "    student_ans = student_lr_reg.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = [ 4.79663712, -3.66143613]\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_lin_reg_compute_gradient(LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VptfbtsMAEVB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa63ee20-06df-4ef8-bc19-b34d6a83977c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 5/5 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_lin_reg_compute_gradient', answer = LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "naEgZPDXQ5_U"
      },
      "outputs": [],
      "source": [
        "def test_lin_reg_has_converged(StudentLinearRegression):\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression()\n",
        "    test_case_theta_old = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_theta_new = np.array([1.624345, -0.611756])\n",
        "    student_ans = student_lr_reg.has_converged(test_case_theta_old, test_case_theta_new)\n",
        "    required_ans = True\n",
        "    \n",
        "    assert student_ans == required_ans\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_lin_reg_has_converged(LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7rx213_3_gmj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2470ed8-f3ce-45ad-b17a-9738f0051295"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 1/1 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_lin_reg_has_converged', answer = LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "NF9jGYOVSrC2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d60f25c-2b5d-403d-de24-572ffa3fdff5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.          0.          0.        ]\n",
            " [ 0.012       0.00566085 -0.00773638]\n",
            " [ 0.02351422  0.01085581 -0.01491529]\n",
            " [ 0.03457102  0.01561393 -0.0215702 ]\n",
            " [ 0.04519706  0.01996249 -0.02773259]\n",
            " [ 0.05541739  0.02392713 -0.03343205]]\n",
            "[[ 0.          0.          0.        ]\n",
            " [ 0.012       0.00566085 -0.00773638]\n",
            " [ 0.02351422  0.01085581 -0.01491529]\n",
            " [ 0.03457102  0.01561393 -0.0215702 ]\n",
            " [ 0.04519706  0.01996249 -0.02773259]\n",
            " [ 0.05541739  0.02392713 -0.03343205]]\n"
          ]
        }
      ],
      "source": [
        "def test_lin_reg_fit(StudentLinearRegression):\n",
        "    \n",
        "    student_lr_reg = StudentLinearRegression(max_iter=5)\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "    student_lr_reg.fit(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_reg.hist_theta_\n",
        "    required_ans = np.array([[ 0.        ,  0.        ,  0.        ],\n",
        "       [ 0.012     ,  0.00566085, -0.00773638],\n",
        "       [ 0.02351422,  0.01085581, -0.01491529],\n",
        "       [ 0.03457102,  0.01561393, -0.0215702 ],\n",
        "       [ 0.04519706,  0.01996249, -0.02773259],\n",
        "       [ 0.05541739,  0.02392713, -0.03343205]])\n",
        "    print(student_ans)\n",
        "    print(required_ans)\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_lin_reg_fit(LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VgCoBChfNobE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bebc00a9-eeca-4273-ef1c-01d76e1e4705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 3/3 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_lin_reg_fit', answer = LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "P0ra9f4IKvy6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60084266-bcc7-4d45-d72e-c8bc0d9b9b57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.          0.          0.        ]\n",
            " [ 0.02        0.03248691 -0.01223513]\n",
            " [ 0.03968062  0.02209216 -0.03335181]\n",
            " [ 0.03696942  0.01974587 -0.02711189]\n",
            " [ 0.03512822  0.01653332 -0.02571035]\n",
            " [ 0.05419193  0.02261539 -0.03046428]\n",
            " [ 0.07200065  0.05154291 -0.04135888]\n",
            " [ 0.09021758  0.04192125 -0.06090506]\n",
            " [ 0.08488414  0.03730565 -0.04862995]\n",
            " [ 0.08114428  0.0307803  -0.04578314]\n",
            " [ 0.09909665  0.03650781 -0.05025993]\n",
            " [ 0.11531376  0.06284999 -0.06018085]\n",
            " [ 0.13237995  0.05383611 -0.07849234]\n",
            " [ 0.12518748  0.04761169 -0.0619386 ]\n",
            " [ 0.1200793   0.03869888 -0.05805022]\n",
            " [ 0.13714127  0.04414231 -0.06230497]\n",
            " [ 0.15220209  0.06860628 -0.07151852]\n",
            " [ 0.16834802  0.06007846 -0.0888426 ]\n",
            " [ 0.15985172  0.05272569 -0.06928804]\n",
            " [ 0.15375991  0.04209663 -0.06465091]\n",
            " [ 0.17009366  0.04730773 -0.06872406]\n",
            " [ 0.18431405  0.07040657 -0.07742348]\n",
            " [ 0.19971005  0.06227484 -0.0939429 ]\n",
            " [ 0.19031372  0.05414318 -0.07231689]\n",
            " [ 0.18351709  0.04228434 -0.06714324]\n",
            " [ 0.19924207  0.04730123 -0.07106459]]\n",
            "[[ 0.          0.          0.        ]\n",
            " [ 0.02        0.03248691 -0.01223513]\n",
            " [ 0.03968062  0.02209216 -0.03335181]\n",
            " [ 0.03696942  0.01974587 -0.02711189]\n",
            " [ 0.03512822  0.01653332 -0.02571035]\n",
            " [ 0.05419193  0.02261539 -0.03046428]\n",
            " [ 0.07200065  0.05154291 -0.04135888]\n",
            " [ 0.09021758  0.04192125 -0.06090506]\n",
            " [ 0.08488414  0.03730565 -0.04862995]\n",
            " [ 0.08114428  0.0307803  -0.04578314]\n",
            " [ 0.09909665  0.03650781 -0.05025993]\n",
            " [ 0.11531376  0.06284999 -0.06018085]\n",
            " [ 0.13237995  0.05383611 -0.07849234]\n",
            " [ 0.12518748  0.04761169 -0.0619386 ]\n",
            " [ 0.1200793   0.03869888 -0.05805022]\n",
            " [ 0.13714127  0.04414231 -0.06230497]\n",
            " [ 0.15220209  0.06860628 -0.07151852]\n",
            " [ 0.16834802  0.06007846 -0.0888426 ]\n",
            " [ 0.15985172  0.05272569 -0.06928804]\n",
            " [ 0.15375991  0.04209663 -0.06465091]\n",
            " [ 0.17009366  0.04730773 -0.06872406]\n",
            " [ 0.18431405  0.07040657 -0.07742348]\n",
            " [ 0.19971005  0.06227484 -0.0939429 ]\n",
            " [ 0.19031372  0.05414318 -0.07231689]\n",
            " [ 0.18351709  0.04228434 -0.06714324]\n",
            " [ 0.19924207  0.04730123 -0.07106459]]\n"
          ]
        }
      ],
      "source": [
        "def test_lin_reg_fit_sgd(StudentLinearRegression):\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(max_iter=5)\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "    student_lr_reg.fit_sgd(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_reg.hist_theta_\n",
        "    required_ans = np.array([[ 0. ,         0. ,         0.        ],\n",
        "        [ 0.02 ,       0.03248691, -0.01223513],\n",
        "        [ 0.03968062,  0.02209216, -0.03335181],\n",
        "        [ 0.03696942,  0.01974587, -0.02711189],\n",
        "        [ 0.03512822,  0.01653332, -0.02571035],\n",
        "        [ 0.05419193,  0.02261539, -0.03046428],\n",
        "        [ 0.07200065,  0.05154291, -0.04135888],\n",
        "        [ 0.09021758,  0.04192125, -0.06090506],\n",
        "        [ 0.08488414,  0.03730565, -0.04862995],\n",
        "        [ 0.08114428,  0.0307803 , -0.04578314],\n",
        "        [ 0.09909665,  0.03650781, -0.05025993],\n",
        "        [ 0.11531376,  0.06284999, -0.06018085],\n",
        "        [ 0.13237995,  0.05383611, -0.07849234],\n",
        "        [ 0.12518748,  0.04761169, -0.0619386 ],\n",
        "        [ 0.1200793 ,  0.03869888, -0.05805022],\n",
        "        [ 0.13714127,  0.04414231, -0.06230497],\n",
        "        [ 0.15220209,  0.06860628, -0.07151852],\n",
        "        [ 0.16834802,  0.06007846, -0.0888426 ],\n",
        "        [ 0.15985172,  0.05272569, -0.06928804],\n",
        "        [ 0.15375991,  0.04209663, -0.06465091],\n",
        "        [ 0.17009366,  0.04730773, -0.06872406],\n",
        "        [ 0.18431405,  0.07040657, -0.07742348],\n",
        "        [ 0.19971005,  0.06227484, -0.0939429 ],\n",
        "        [ 0.19031372,  0.05414318, -0.07231689],\n",
        "        [ 0.18351709,  0.04228434, -0.06714324],\n",
        "        [ 0.19924207,  0.04730123, -0.07106459]])\n",
        "    print(student_ans)\n",
        "    print(required_ans)\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_lin_reg_fit_sgd(LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "sewbA8dC5Fdm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "427f8a2c-93b9-41ce-8f0b-1899ef480b22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 3/3 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_lin_reg_fit_sgd', answer = LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "uQQN2ky4UYxx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04df341c-d218-4b0b-d078-84eddee5ccbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "student_ans [0.04739416 0.02735934 0.02140787 0.04634383 0.04320043 0.02836861\n",
            " 0.03726417 0.03808224 0.03214353 0.05166998 0.05102933 0.05639199\n",
            " 0.0416892  0.03175554 0.04895695 0.03465034 0.02912364 0.03954521\n",
            " 0.0396391  0.06440433 0.03189335 0.06016748 0.03661307 0.07146111\n",
            " 0.05261461 0.04180017 0.03223834 0.0500466  0.06128615 0.05703506\n",
            " 0.05467262 0.04388664 0.04648138 0.07052753 0.04140456 0.02830984\n",
            " 0.05608863 0.0212115  0.05238969 0.05514024 0.04020117 0.05048966\n",
            " 0.04696158 0.04438422 0.05897309 0.05443805 0.03375689 0.04794345\n",
            " 0.04242038 0.04869202]\n"
          ]
        }
      ],
      "source": [
        "def test_lin_reg_predict(StudentLinearRegression):\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(max_iter=5)\n",
        "    np.random.seed(1)\n",
        "    test_case_X = np.random.randn(50, 2)\n",
        "    test_case_y = np.random.randint(0, 2, 50)\n",
        "    student_lr_reg.fit(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_reg.predict(test_case_X)\n",
        "    print('student_ans', student_ans)\n",
        "    required_ans = np.array([0.04739416, 0.02735934, 0.02140787, 0.04634383, 0.04320043,\n",
        "       0.02836861, 0.03726417, 0.03808224, 0.03214353, 0.05166998,\n",
        "       0.05102933, 0.05639199, 0.0416892 , 0.03175554, 0.04895695,\n",
        "       0.03465034, 0.02912364, 0.03954521, 0.0396391 , 0.06440433,\n",
        "       0.03189335, 0.06016748, 0.03661307, 0.07146111, 0.05261461,\n",
        "       0.04180017, 0.03223834, 0.0500466 , 0.06128615, 0.05703506,\n",
        "       0.05467262, 0.04388664, 0.04648138, 0.07052753, 0.04140456,\n",
        "       0.02830984, 0.05608863, 0.0212115 , 0.05238969, 0.05514024,\n",
        "       0.04020117, 0.05048966, 0.04696158, 0.04438422, 0.05897309,\n",
        "       0.05443805, 0.03375689, 0.04794345, 0.04242038, 0.04869202])\n",
        "    \n",
        "    assert np.mean(np.abs(student_ans - required_ans)) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_lin_reg_predict(LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "LvZwTzBBN00l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3afa8015-2c72-4ace-c8d8-c5b4187aabff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 1/1 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_lin_reg_predict', answer = LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "UIACk5LZLqz7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1d37c8-6728-4245-cacd-f4c58d9769d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "student_ans [0.4113478  0.28834918 0.1227324  0.39008601 0.43987045 0.17506316\n",
            " 0.40365951 0.32180596 0.32776898 0.56721846 0.63147595 0.57385561\n",
            " 0.38334306 0.31959516 0.5517445  0.39322627 0.3213112  0.45537132\n",
            " 0.48490982 0.62956115 0.32575875 0.72747134 0.37152396 0.81428507\n",
            " 0.57451273 0.42292006 0.3905908  0.56212164 0.64126265 0.62130162\n",
            " 0.65671342 0.43645374 0.47163355 0.74245718 0.29808437 0.35882346\n",
            " 0.61700668 0.15509352 0.59866825 0.60026664 0.43537041 0.5427557\n",
            " 0.49628385 0.51805151 0.65681787 0.52965323 0.36155917 0.49471154\n",
            " 0.47184886 0.57066729]\n"
          ]
        }
      ],
      "source": [
        "def test_lin_reg_predict_sgd(StudentLinearRegression):\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(max_iter=5)\n",
        "    np.random.seed(1)\n",
        "    test_case_X = np.random.randn(50, 2)\n",
        "    test_case_y = np.random.randint(0, 2, 50)\n",
        "    student_lr_reg.fit_sgd(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_reg.predict(test_case_X)\n",
        "    print('student_ans', student_ans)\n",
        "    required_ans = np.array([0.4113478,  0.28834918, 0.1227324,  0.39008601, 0.43987045, 0.17506316,\n",
        "                            0.40365951, 0.32180596, 0.32776898, 0.56721846, 0.63147595, 0.57385561,\n",
        "                            0.38334306, 0.31959516, 0.5517445,  0.39322627, 0.3213112,  0.45537132,\n",
        "                            0.48490982, 0.62956115, 0.32575875, 0.72747134, 0.37152396, 0.81428507,\n",
        "                            0.57451273, 0.42292006, 0.3905908,  0.56212164, 0.64126265, 0.62130162,\n",
        "                            0.65671342, 0.43645374, 0.47163355, 0.74245718, 0.29808437, 0.35882346,\n",
        "                            0.61700668, 0.15509352, 0.59866825, 0.60026664, 0.43537041, 0.5427557,\n",
        "                            0.49628385, 0.51805151, 0.65681787, 0.52965323, 0.36155917, 0.49471154,\n",
        "                            0.47184886, 0.57066729])\n",
        "    \n",
        "    assert np.mean(np.abs(student_ans - required_ans)) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_lin_reg_predict_sgd(LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "WKhfPR6uUJm9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7a4f04d-f455-43c3-83b2-e1051bdaf7b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 1/1 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_lin_reg_predict_sgd', answer = LinearRegression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGlB3lojik_m"
      },
      "source": [
        "## **1.2. Synthetic dataset [Ungraded]**\n",
        "\n",
        "In this section we will first create some synthetic data on which we will run your linear regression implementation. We are creating 100 datapoints around the function y = mx + b, introducing Gaussian noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Rqp2jLGTiJQo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "2942a255-bdfb-4f82-cb50-e6e4e2d8ef35"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Don't modify this cell\n",
        "if NOTEBOOK:\n",
        "    num_samples = 100\n",
        "\n",
        "    np.random.seed(1)\n",
        "    noise = np.random.randn(num_samples, 1)\n",
        "    X = np.random.randn(num_samples, 1)\n",
        "\n",
        "    y_ideal = 11*X + 5\n",
        "    y_real = (11*X + 5) + noise\n",
        "\n",
        "    plt.plot(X, y_real, 'ro')\n",
        "    plt.plot(X, y_ideal, 'b')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G9qQh_uiXQt"
      },
      "source": [
        "We see that this data is clearly regressable with a line, which, ideally, would be 11x + 5\n",
        "\n",
        "Train a linear regression model using gradient descent, you should see that training loss goes down with the number of iterations and obtain a theta that converges to a value very close to [b, m], which in this case, for 11x + 5, would be theta = [5, 11]\n",
        "\n",
        "Also, notice the effect of the type of regularization on the theta obtained (after convergence) as well as the testing MSE loss. Do they make sense, given what was discussed in class? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "MYzOlitsiNCo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "201dcc73-7445-4978-c6a0-ab67addc6e05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-83f156cd7e53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mNOTEBOOK\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mtest_synthetic_data_sgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ideal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mtest_synthetic_data_sgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ideal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"l1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mtest_synthetic_data_sgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ideal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"l2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-83f156cd7e53>\u001b[0m in \u001b[0;36mtest_synthetic_data_sgd\u001b[0;34m(X, y, n_iter, penalty, lambd)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" Theta: {} \\n Norm of Theta: {} \\n Testing MSELoss: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mloss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist_cost_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2609\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2610\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2611\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Improper number of dimensions to norm.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Improper number of dimensions to norm."
          ]
        }
      ],
      "source": [
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def test_synthetic_data_sgd(X, y, n_iter = 2000, penalty=None, lambd=0):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=37)\n",
        "  # Given that we want to get theta as the weights of the linear equation, we won't \n",
        "  # standardize in this section\n",
        "\n",
        "  alpha = 0.03  # Learning Rate\n",
        "\n",
        "  # # Train the model\n",
        "  lr_model = LinearRegression(alpha = alpha, tol=1e-4, max_iter = n_iter, penalty=penalty, lambd=lambd)\n",
        "  lr_model.fit(X_train,y_train[:, 0])\n",
        "  y_predict = lr_model.predict(X_test)\n",
        "  loss = sklearn.metrics.mean_squared_error(y_predict, y_test)\n",
        "  print()\n",
        "  print(\" Theta: {} \\n Norm of Theta: {} \\n Testing MSELoss: {}\".format(lr_model.theta_, np.linalg.norm(lr_model.theta_, ord=2), loss))\n",
        "\n",
        "  loss_history = lr_model.hist_cost_\n",
        "  plt.plot(range(len(loss_history)), loss_history)\n",
        "  plt.title(\"OLS Training Loss\")\n",
        "  plt.xlabel(\"iteration\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  if penalty == \"l1\":\n",
        "    plt.title(\"L1 Regularised Training Loss\")\n",
        "  elif penalty == \"l2\":\n",
        "    plt.title(\"L2 Regularised Training Loss\")\n",
        "  plt.show()\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_synthetic_data_sgd(X, y_ideal, 500)\n",
        "    test_synthetic_data_sgd(X, y_ideal, 500, \"l1\", 0.02)\n",
        "    test_synthetic_data_sgd(X, y_ideal, 500, \"l2\", 0.02)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qUcFLi6tjL_"
      },
      "source": [
        "## **1.3. Effect of polynomial degree on training and validation error [5 pts, manually graded]**\n",
        "\n",
        "Now, we consider a dataset that was generated using some higher degree polynomial function of the input variable. We do not know the degree of the underlying polynomial. Let us assume it to be an unknown value \"p\" and try to estimate it.\n",
        "\n",
        "Polynomial regression hypothesis for one input variable  or feature (x) can be written as:\n",
        "> $y = w_0 + w_1x + w_2x^2 + ... + w_px^p $\n",
        "\n",
        "If you observe carefully, this can still be solved as a linear regression, where, instead of just 2 weights, we have p+1 weights, and the new features are higher order terms of the original feature. Using this idea, in this section, we will investigate how changing the assumed polynomial degree \"p\" in our model affects the training and validation error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "UcpXD4pTarFC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe2b1eb0-cde0-4fea-9880-0f01f766c56d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1F5cGpc9ayxf49RQskiJFlL0v5LzCP7dn\n",
            "To: /content/cis519_hw2_poly_reg.csv\n",
            "100% 50.7k/50.7k [00:00<00:00, 41.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "if NOTEBOOK:\n",
        "    \n",
        "    if not os.path.exists(\"cis519_hw2_poly_reg.csv\"):\n",
        "        !gdown --id 1F5cGpc9ayxf49RQskiJFlL0v5LzCP7dn\n",
        "    \n",
        "    poly_reg_df = pd.read_csv('cis519_hw2_poly_reg.csv')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "qJMhCZ6SuepA"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression as LinearRegressionSklearn\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def polynomial_regression(poly_reg_df, degrees):\n",
        "    \"\"\"\n",
        "    Runs polynomial regression on the dataset 'poly_reg_df' for all the powers in 'degrees'\n",
        "    \"\"\"\n",
        "\n",
        "    loss_train_list = []\n",
        "    loss_test_list = []\n",
        "\n",
        "    X_base = poly_reg_df.iloc[:, :-1].values\n",
        "    y = poly_reg_df.iloc[:, -1].values\n",
        "\n",
        "    for d in degrees:\n",
        "\n",
        "        # TODO START: Complete the function:\n",
        "        # 1. Transform the base feature X_base into its polynomial features of degree 'd' using PolynomialFeatures\n",
        "        # Set include_bias to be False\n",
        "        poly_features = PolynomialFeatures(degree=d, include_bias=False)\n",
        "        X = poly_features.fit_transform(X_base)\n",
        "        \n",
        "        # 2. Preprocessing and splitting into train/test (70-30 ratio and random_state as 42)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "        \n",
        "        # 3. Scale X_train and X_test appropriately \n",
        "        sc = StandardScaler()\n",
        "        X_train = sc.fit_transform(X_train)\n",
        "        X_test = sc.transform(X_test)\n",
        "        \n",
        "        # 4. Use scikit-learn's LinearRegression (imported as LinearRegressionSklearn for you) to \n",
        "        # fit a linear model between the scaled version of X_train and y_train\n",
        "        regressor = LinearRegressionSklearn()\n",
        "        regressor.fit(X_train, y_train)\n",
        "        \n",
        "        # 5. Obtain predictions of the model on train and test data\n",
        "        y_train_pred = regressor.predict(X_train)\n",
        "        y_test_pred = regressor.predict(X_test)\n",
        "        \n",
        "        # 6. Compute the mean squared error and store it in loss_train and loss_test\n",
        "        loss_train = mean_squared_error(y_train, y_train_pred)\n",
        "        loss_test = mean_squared_error(y_test, y_test_pred)\n",
        "        \n",
        "        # 7. Append loss_train to loss_train_list and loss_test to loss_test_list\n",
        "        loss_train_list.append(loss_train)\n",
        "        loss_test_list.append(loss_test)\n",
        "        \n",
        "    return loss_train_list, loss_test_list\n",
        "    # TODO END"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tPvFq_bdvohP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "aDiIc_XjuBqk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "2dacbe16-11bc-4c49-cdba-1e85e29906bd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if NOTEBOOK:\n",
        "\n",
        "    degrees = np.arange(1, 9)\n",
        "    \n",
        "    loss_train_list, loss_test_list = polynomial_regression(poly_reg_df, degrees)\n",
        "\n",
        "    # TODO START:\n",
        "    # Plot the polynomial degrees (x-axis) against loss_train_list (y-axis) and loss_test_list (y-axis) in a single plot, with different colors.\n",
        "    # Make sure to include x and y axis labels, legend as well as the title\n",
        "    plt.plot(degrees, loss_train_list, label='Train Loss')\n",
        "    plt.plot(degrees, loss_test_list, label='Test Loss')\n",
        "    plt.xlabel('Polynomial Degree')\n",
        "    plt.ylabel('Mean Squared Error')\n",
        "    plt.legend()\n",
        "    plt.title('Polynomial Regression Loss')\n",
        "    plt.show()\n",
        "    # TODO END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-ByMGwo1TBS"
      },
      "source": [
        "**Attach the plot to your written homework solutions. Describe the trends in the plot obtained. Briefly explain the reasoning behind why this would happen.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TLTJhmiOr7X"
      },
      "source": [
        "## **1.4. Effect of learning rate on gradient descent [5 pts, manually graded]**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxNxtdHnRLws"
      },
      "source": [
        "Run the below cell to download the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "c6W4LVZgMl3g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a86f3b7e-3dc5-49b1-e521-9634eff760c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CSD1vA9qZucuevxCuaOwr91tBaZcjNNh\n",
            "To: /content/cis519_hw2_admit.csv\n",
            "100% 3.61k/3.61k [00:00<00:00, 1.88MB/s]\n"
          ]
        }
      ],
      "source": [
        "if NOTEBOOK:\n",
        "    \n",
        "    if not os.path.exists(\"cis519_hw2_admit.csv\"):\n",
        "        !gdown --id 1CSD1vA9qZucuevxCuaOwr91tBaZcjNNh\n",
        "    \n",
        "    train_df = pd.read_csv(\"cis519_hw2_admit.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqzaeWj1PIa-"
      },
      "source": [
        "The dataset contains two features - scores in two exams and the target variable is whether the student was admitted into a college or not. Your task for this question is to use this dataset and plot the variation of cost function with respect to the number of gradient descent iterations for different learning rates. Perform the following steps.\n",
        "\n",
        "1. Scale the features using StandardScaler\n",
        "2. For each of the learning rates - {0.001, 0.01, 0.03, 0.1, 1.0}, fit a linear regression model to the scaled data by running a maximum of 100 iterations of gradient descent with L2 penalty and $\\lambda$ as 0.001.\n",
        "3. Show the variation of the cost (stored in `hist_cost_`) with respect to the number of iterations for all the learning rates in the same plot.\n",
        "\n",
        "Submit the plot along with the written homework solutions. The plot should have an appropriate title, axes labels, and legend. Briefly comment on the effect of increasing learning rate and what would be the best learning rate among the four values based on the plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "5xbTy8vxAaYf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "outputId": "e7fafa2d-86bc-4e59-8f88-8b23ab3d9ec2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x1152 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# TODO: The previously the plot showed a divergent behavior for alpha=0.25, but with linear regression, we do not find any learning rate that exhibits such behavior\n",
        "\n",
        "if NOTEBOOK:\n",
        "    # STUDENT CODE STARTS:\n",
        "    train_df.columns = ['score1', 'score2', 'admit']\n",
        "    X = train_df[['score1', 'score2']]\n",
        "    y = train_df['admit']\n",
        "    scaler = StandardScaler()\n",
        "    X_std = scaler.fit_transform(X)\n",
        "    learn = [0.001, 0.01, 0.03, 0.1, 1]\n",
        "    plt.figure(figsize = (8,16))\n",
        "    for i in range(len(learn)):\n",
        "      lr = learn[i]\n",
        "      LR_model = LinearRegression(max_iter=100,tol=0.0001,penalty = \"l2\", lambd = 0.001, alpha = lr)\n",
        "      LR_model.fit(X_std, y)\n",
        "      plt.plot(LR_model.hist_cost_, label = \"lr : {}\".format(lr))\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"iterations\")\n",
        "    plt.ylabel(\"cost\")\n",
        "    plt.title(\"cost of linear regression with different learning rates(lr)\")\n",
        "    plt.yticks(np.arange(0, 7.5, 0.25))\n",
        "    # STUDENT CODE ENDS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYRykDPr9U8A"
      },
      "source": [
        "Download the .ipynb notebook and submit on Gradescope."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "377c3e77380f886ab555d62b93e59a1648fc55affccd8d0220be3281f77f4c6d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}